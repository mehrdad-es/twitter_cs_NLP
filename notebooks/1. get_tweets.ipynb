{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "import sys   \n",
    "from selenium.webdriver import Keys, ActionChains\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "import time\n",
    "import os    \n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import calendar as cl\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1,'../src')\n",
    "import general_tools as gt\n",
    "import get_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_search_term='iphone 6'\n",
    "date = '2/14/2015'\n",
    "months=['Jan ','Feb ','Mar ','Apr ','May ','Jun ','Jul ','Aug ','Sep ','Oct ','Nov ','Dec ']\n",
    "config= pd.read_csv('../bucket/config/config.csv',index_col=0)\n",
    "phone_model,date=exact_search_term,gt.analyze_date_string(date)\n",
    "day_after = date+dt.timedelta(days=1)\n",
    "year,month,day=date.year,date.month,date.day\n",
    "day_after_year,day_after_month,day_after_day=day_after.year,day_after.month,day_after.day\n",
    "scan_summary = pd.read_csv(config.loc['iphone6_scan_summary'][0])\n",
    "all_tweets_repo = pd.read_csv(config.loc['iphone6_tweets_repository'][0])\n",
    "today = dt.datetime.now().strftime('%Y %m %d | %H:%M')\n",
    "repo=[]\n",
    "time_start=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To input your user credentials so you can use it in code\n",
    "\n",
    "# os.environ['email']=\n",
    "# os.environ['username']=\n",
    "# os.environ['password']=\n",
    "# os.environ['unexpected_twitter_login_code']="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "opens a chrome instance. Note that local chrome version and the chrome driver\n",
    "in the working directory should match. Also note that selenium webscraping \n",
    "is finicky and needs continuous updating due to code changes or front end changes \n",
    "on the website being scraped. \n",
    "'''\n",
    "chrome_options = ChromeOptions()\n",
    "# chrome_options.add_experimental_option(\"detach\", True) # to sometimes help with chrome tab stay open \n",
    "# chrome_options.add_argument(\"--headless=new\") # so chrome doesn't open every single time\n",
    "# chrome_options.add_experimental_option(\"debuggerAddress\",\"localhost:9014\") # alternative way to decalre port for created profile\n",
    "# chrome_options.add_argument(\"--remote-debugging-port=9014\")#opens created profile not to login again\n",
    "# chrome_options.add_argument('--user-data-dir=../tw_profile')#better change to absolute path if encountered errors # this line opens your profile which is already logged into twitter\n",
    "service = Service(executable_path=\"../chromedriver\") # this is based on how this github repo is structured (i.e. chromedirver is in main directory alongside README.md)\n",
    "driver = webdriver.Chrome(service=service,options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eml = os.environ.get('email')\n",
    "usr=os.environ.get('username')\n",
    "psw=os.environ.get('password')\n",
    "unq=os.environ.get('unexpected_twitter_login_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver = get_tweets.initiate_chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_tweets.login_to_twitter(driver,eml,usr,psw,unq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_tweets.scrape_twitter(driver,'2/15/2015',threshold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged into twitter\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "loads in user credentials and logs into twitter\n",
    "'''\n",
    "\n",
    "eml = os.environ.get('email')\n",
    "usr=os.environ.get('username')\n",
    "psw=os.environ.get('password')\n",
    "unq=os.environ.get('unexpected_twitter_login_code')\n",
    "driver.implicitly_wait(15)\n",
    "# login = driver.find_element(By.XPATH, '//span[contains(text(),\"Log in\")]')\n",
    "# login.click()\n",
    "time.sleep(2) # for popup to load cause it can be unreliable\n",
    "popup=driver.find_element(By.XPATH,\"//div[@aria-labelledby='modal-header' and @role='dialog' and contains(.,'Sign in')]\")\n",
    "username = driver.find_element(By.CSS_SELECTOR, \"input[autocomplete='username']\")\n",
    "username.clear()\n",
    "username.send_keys(eml)\n",
    "username.send_keys(Keys.ENTER) #change this selecting next button #betterENG\n",
    "time.sleep(2) #for selenium to update what popup is\n",
    "if 'There was unusual' in popup.text:\n",
    "    input=driver.find_element(By.CSS_SELECTOR,'input[name=\"text\"]')#betterENG\n",
    "    input.send_keys(usr)\n",
    "    input.send_keys(Keys.ENTER)\n",
    "password=driver.find_element(By.CSS_SELECTOR,'input[name=\"password\"]')\n",
    "password.send_keys(psw)\n",
    "driver.find_element(By.CSS_SELECTOR,'div[role=\"button\"][data-testid=\"LoginForm_Login_Button\"]').click()\n",
    "time.sleep(2)\n",
    "print('Logged into twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapes twitter\n",
    "time_start=time.time()    \n",
    "driver.get('https://twitter.com')\n",
    "time.sleep(2)\n",
    "explore=driver.find_element(By.CSS_SELECTOR,'a[href=\"/explore\"][aria-label=\"Search and explore\"]')\n",
    "#width of the ec2 instance is 700px while height is 400px\n",
    "if explore.size!=0:\n",
    "    # print(explore.size,explore.is_displayed())\n",
    "    explore.click()\n",
    "    input = driver.find_element(By.CSS_SELECTOR,'input[aria-label=\"Search query\"]')\n",
    "else:\n",
    "    input = driver.find_element(By.CSS_SELECTOR,'input[aria-label=\"Search query\"]')\n",
    "advanced_search=f'\"{phone_model}\" (lang:en) until:{day_after_year}-{day_after_month}-{day_after_day} since:{year}-{month}-{day}'\n",
    "input.send_keys(advanced_search)\n",
    "input.send_keys(Keys.ENTER)\n",
    "latest = driver.find_element(By.CSS_SELECTOR,'a[href^=\"/search?q\"][role=\"tab\"][href$=\"f=live\"]')\n",
    "latest.click()\n",
    "y_0=driver.execute_script(\"return document.body.scrollHeight\")\n",
    "count=0\n",
    "count2=1\n",
    "threshold=11# this would be a function input\n",
    "if_no_articles=bool(len(driver.find_elements(By.XPATH,f'//div[contains(.,\"No results for\") and contains(.,\"Try searching for something else\")]')))\n",
    "if if_no_articles==False:\n",
    "    while count2<=threshold:\n",
    "        tweet_elements=driver.find_elements(By.TAG_NAME,'article')\n",
    "        for i in range(len(tweet_elements)):\n",
    "            if tweet_elements[i].text=='This Tweet is unavailable.':\n",
    "                continue\n",
    "            text=''\n",
    "            elements=tweet_elements[i].text.split('\\n')\n",
    "            if '·' not in elements[:4]:\n",
    "                continue\n",
    "            else:\n",
    "                elements.remove('·')\n",
    "                for k in range(1,len(elements)):\n",
    "                        if elements[k][:4] in months:\n",
    "                            date_index= k\n",
    "                            break\n",
    "                for j in range(date_index+1,len(elements)):\n",
    "                    text+=elements[j]    \n",
    "                row = elements[:date_index+1]\n",
    "                tw_date= dt.datetime.strptime(row[date_index],'%b %d, %Y')\n",
    "                if len(row)==2:\n",
    "                    row.insert(0,'No Name')    \n",
    "                row.append(text)\n",
    "                if row not in repo:\n",
    "                    repo.append(row)\n",
    "                    count2+=1\n",
    "            if count2==threshold:\n",
    "                break\n",
    "        count+=1\n",
    "        driver.execute_script(\"window.scrollTo(0, {});\".format(count*y_0))\n",
    "        time.sleep(3)\n",
    "        if_pls_retry=bool(len(driver.find_elements(By.XPATH,f'//div[contains(.,\"Something went wrong. Try reloading\") and contains(.,\"Retry\")]')))\n",
    "        if if_pls_retry:\n",
    "            driver.find_element(By.XPATH,f'//div[@role=\"button\" and contains(.,\"Retry\")]').click()\n",
    "        y_1= driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if y_1==y_0:\n",
    "            time_of_break = time.time()\n",
    "            break\n",
    "        y_0=y_1       \n",
    "else:\n",
    "    scan_summary.loc[scan_summary.shape[0]]=['success/no results',today,time.time()-time_start,'None']\n",
    "    scan_summary.to_csv(config.loc['iphone6_scan_summary'][0])            \n",
    "    print('break')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "* in 3 hours about 5000 english tweets were captured globably that specifically mentioned iphone6. This number is really large\n",
    "* Every 800 tweets or so the scrolling feature in twitter stops working for 5-10 mins. This slows down the scan\n",
    "* Due to latest tab being choronologically ordered, and the large number of tweets, only tweets during night time could be captured after an hour of scraping (9pm to 12pm)\n",
    "\n",
    "*As a result, only 500 tweets for each day is going to be grabbed for better scrape speed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_df=pd.DataFrame(repo,columns=['name','id','date','text'])\n",
    "all_tweets_repo = pd.concat([all_tweets_repo,repo_df],ignore_index=True)\n",
    "all_tweets_repo.to_csv(config.loc['iphone6_tweets_repository'][0],index=False)\n",
    "all_tweets_repo.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_end=time.time()\n",
    "process_runtime=time_end-time_start\n",
    "scan_summary.loc[scan_summary.shape[0]]= ['success',today,process_runtime,'None']\n",
    "scan_summary.to_csv(config.loc['iphone6_scan_summary'][0],index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
